{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e13a085-82ff-4b70-bd97-5ee560d49d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b59a4e3-138c-45ab-9812-770b6d634eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo en español de Spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Definir la oración a analizar\n",
    "#sentence = \"La inflación del Imperio Romano\"\n",
    "sentence = \"B) \"\"CENTESIMUS ANNUS\"\". Reflexiones de un economista sobre la doctrina social de la Iglesia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0663b327-23ed-404f-8197-55f8e0600c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B) CENTESIMUS ANNUS. Reflexiones de un economista sobre la doctrina social de la Iglesia'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a6a8bf2-72d1-4587-a667-5694cfbadbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B - PROPN\n",
      ") - PUNCT\n",
      "CENTESIMUS - PROPN\n",
      "ANNUS - PROPN\n",
      ". - PUNCT\n",
      "Reflexiones - NOUN\n",
      "de - ADP\n",
      "un - DET\n",
      "economista - NOUN\n",
      "sobre - ADP\n",
      "la - DET\n",
      "doctrina - NOUN\n",
      "social - ADJ\n",
      "de - ADP\n",
      "la - DET\n",
      "Iglesia - PROPN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(sentence)\n",
    "for token in doc:\n",
    "    print(token.text + ' - ' + token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2608c3ef-3762-4dcd-9948-32e29d54edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the noun phrases from the document\n",
    "noun_phrases = []\n",
    "for chunk in doc.noun_chunks:\n",
    "    if len(chunk) >= 1:\n",
    "        noun_phrases.append(chunk)\n",
    "\n",
    "# Extract the individual nouns from the noun phrases\n",
    "nouns = []\n",
    "for phrase in noun_phrases:\n",
    "    for token in phrase:\n",
    "        if len(token) > 1 and (token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\") :\n",
    "            tokens = [token.text.lower() for token in phrase if not token.is_stop and not token.is_punct]      \n",
    "            new_phrase = \" \".join(tokens)\n",
    "            if new_phrase != \"\":\n",
    "                nouns.append(new_phrase)\n",
    "                break\n",
    "\n",
    "# Print the result\n",
    "print(nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ab783-0997-432f-95bd-384da29ceb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "compound_nouns = []\n",
    "for i, token in enumerate(doc):\n",
    "    if token.pos_ in [\"NOUN\", \"ADJ\", \"PROPN\"]:\n",
    "        breakpoint()\n",
    "        if i < len(doc)-2 and doc[i+1].pos_ in [\"NOUN\", \"ADJ\", \"PROPN\"] or doc[i+2].pos_ in [\"NOUN\", \"ADJ\", \"PROPN\"]:\n",
    "            print(token.text)\n",
    "            compound_nouns.append((token.text, doc[i+2].text))\n",
    "\n",
    "print(compound_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7d7728d-4a89-4dbe-8b56-67dce8975a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/var/folders/80/s813wmn56kl8pmtl1sq468840000gn/T/ipykernel_50945/4124637924.py\u001b[0m(5)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      3 \u001b[0;31m\u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoun_chunks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      4 \u001b[0;31m    \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 5 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      6 \u001b[0;31m        \u001b[0mnoun_phrases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      7 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  chunk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La inflación\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  doc.noun_chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object at 0x182d4bf40>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  chunk.text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'La inflación'\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/var/folders/80/s813wmn56kl8pmtl1sq468840000gn/T/ipykernel_50945/4124637924.py\u001b[0m(6)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      4 \u001b[0;31m    \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      5 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 6 \u001b[0;31m        \u001b[0mnoun_phrases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      7 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      8 \u001b[0;31m\u001b[0;31m# Extract the individual nouns from the noun phrases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/var/folders/80/s813wmn56kl8pmtl1sq468840000gn/T/ipykernel_50945/4124637924.py\u001b[0m(4)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      2 \u001b[0;31m\u001b[0mnoun_phrases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      3 \u001b[0;31m\u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoun_chunks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 4 \u001b[0;31m    \u001b[0mbreakpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      5 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      6 \u001b[0;31m        \u001b[0mnoun_phrases\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  chunk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imperio Romano\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inflación']\n"
     ]
    }
   ],
   "source": [
    "# Extract the noun phrases from the document\n",
    "noun_phrases = []\n",
    "for chunk in doc.noun_chunks:\n",
    "    breakpoint()\n",
    "    if len(chunk) > 1:\n",
    "        noun_phrases.append(chunk.text)\n",
    "\n",
    "# Extract the individual nouns from the noun phrases\n",
    "nouns = []\n",
    "for phrase in noun_phrases:\n",
    "    phrase_doc = nlp(phrase)\n",
    "    for token in phrase_doc:\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            nouns.append(token.text)\n",
    "\n",
    "# Print the result\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a91cc09b-f4a7-41ee-af48-96db2f65c8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Progreso y crisis en el cambio histórico\"\n",
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8e2d68f-25bd-44c0-871a-23ca93d69547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progreso\n",
      "crisis\n",
      "el cambio\n",
      "Progreso - PROPN\n",
      "y - CCONJ\n",
      "crisis - NOUN\n",
      "en - ADP\n",
      "el - DET\n",
      "cambio - NOUN\n",
      "histórico - ADJ\n"
     ]
    }
   ],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print( chunk)\n",
    "for token in doc:\n",
    "    print(token.text + ' - ' + token.pos_)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9410b24-f11f-4968-91d2-9e216fb58d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
